<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Data</title>
</head>
<body>
<table border=1>
<tr>
<td bgcolor=silver class='medium'>PaperID</td>
<td bgcolor=silver class='medium'>Title</td>
<td bgcolor=silver class='medium'>Abstract</td>
<td bgcolor=silver class='medium'>Year</td>
<td bgcolor=silver class='medium'>Conference</td>
</tr>

<tr>
<td class='normal' valign='top'>1</td>
<td class='normal' valign='top'>Trusting Information Sources One Citizen at a Time</td>
<td class='normal' valign='top'>This paper describes an approach to derive assessments about information        sources based on individual feedback about the sources. We describe        TRELLIS, a system that helps users annotate their analysis of        alternative information sources that can be contradictory and        incomplete. As the user makes a decision on which sources to dismiss and        which to believe in making a final decision, TRELLIS captures the        derivation of the decision in a semantic markup. TRELLIS then uses these        annotations to derive an assessment of the source based on the        annotations of many individuals. Our work builds on the Semantic Web and        presents a tool that helps users create annotations that are in a mix of        formal and human language, and exploits the formal representations to        derive measures of trust in the content of Web resources and their        original source.</td>
<td class='normal' valign='top'>2002</td>
<td class='normal' valign='top'>23541</td>
</tr>

<tr>
<td class='normal' valign='top'>2</td>
<td class='normal' valign='top'>Automatic Generation of Java/SQL based Inference Engines from RDF Schema and RuleML</td>
<td class='normal' valign='top'>This paper describes two approaches for automatically converting RDF Schema  and RuleML sources into an inference engine and storage repository. Rather than  using traditional inference systems, our solution bases on mainstream  technologies like Java and relational database systems. While this necessarily  imposes some restrictions, the ease of integration into an existing IT landscape  is a major advantage. We present the conversion tools and their limitations.  Furthermore, an extension to RuleML is proposed, that allows Java-enabled  reaction rules, where calls to Java libraries can be performed upon a rule  firing. This requires hosts to be Java-enabled when rules and code are moved  across the web. However, the solution allows for great engineering  flexibility.</td>
<td class='normal' valign='top'>2002</td>
<td class='normal' valign='top'>23541</td>
</tr>

<tr>
<td class='normal' valign='top'>4</td>
<td class='normal' valign='top'>Three Implementations of SquishQL, a Simple RDF Query Language</td>
<td class='normal' valign='top'>RDF provides a basic way to represent data for the Semantic Web. We have        been experimenting with the query paradigm for working with RDF data in        semantic web applications. Query of RDF data provides a declarative        access mechanism that is suitable for application usage and remote        access. We describe work on a conceptual model for querying RDF data        that refines ideas first presented in at the W3C workshop on Query        Languages and the design of one possible syntax, derived from rdfDB,        that is suitable for application programmers. Further, we present        experience gained in three implementations of the query language.</td>
<td class='normal' valign='top'>2003</td>
<td class='normal' valign='top'>23541</td>
</tr>

<tr>
<td class='normal' valign='top'>5</td>
<td class='normal' valign='top'>A Data Integration Framework for E-commerce Product Classification</td>
<td class='normal' valign='top'>A marketplace is        the place in which the demand and supply of buyers and vendors        participating in a business process may meet. Therefore, electronic        marketplaces are virtual communities in which buyers may meet proposals        of several suppliers and make the best choice. In the electronic        commerce world, the comparison between different products is blocked due        to the lack of standards (on the contrary, the proliferation of        standards) describing and classifying them. Therefore, the need for B2B        and B2C marketplaces is to reclassify products and goods according to        different standardization models. This paper aims to face this problem        by suggesting the use of a semi-automatic methodology, supported by a        tool (SI-Designer), to define the mapping among different e-commerce        product classification standards. This methodology was developed for the        MOMIS-system within the Intelligent Integration of Information research        area. We describe our extension to the methodology that makes it        applyable in general to product classification standard, by selecting a        fragment of ECCMA/UNSPSC and ecl@ss standard.</td>
<td class='normal' valign='top'>2002</td>
<td class='normal' valign='top'>23541</td>
</tr>

<tr>
<td class='normal' valign='top'>6</td>
<td class='normal' valign='top'>Integrating Vocabularies: Discovering and Representing Vocabulary Maps</td>
<td class='normal' valign='top'>The Semantic Web would enable new ways of doing business on the&lt;br&gt;Web        that require development of advanced business document&lt;br&gt;integration        technologies performing intelligent document&lt;br&gt;transformation. The        documents use different vocabularies that&lt;br&gt;consist of large        hierarchies of terms. Accordingly, vocabulary&lt;br&gt;mapping and        transformation becomes an important task in the whole&lt;br&gt;business        document transformation process. It includes several&lt;br&gt;subtasks: map        discovery, map representation, and map execution&lt;br&gt;that must be        seamlessly integrated into the document integration&lt;br&gt;process. In this        paper we discuss the process of discovering the&lt;br&gt;maps between two        vocabularies assuming availability of two sets of&lt;br&gt;documents, each        using one of the vocabularies. We take the&lt;br&gt;vocabularies of product        classification codes as a playground and&lt;br&gt;propose a reusable map        discovery technique based on Bayesian text&lt;br&gt;classification approach.        We show how the discovered maps can be&lt;br&gt;integrated into the document        transformation process.</td>
<td class='normal' valign='top'>2003</td>
<td class='normal' valign='top'>23541</td>
</tr>

<tr>
<td class='normal' valign='top'>7</td>
<td class='normal' valign='top'>The Semantic Web Revisited</td>
<td class='normal' valign='top'>The original Scientific American article on the Semantic Web appeared in 2001. It described the evolution of a Web that consisted largely of documents for humans to read to one that included data and information for computers to manipulate. The Semantic Web is a Web of actionable information--information derived from data through a semantic theory for interpreting the symbols.This simple idea, however, remains largely unrealized. Shopbots and auction bots abound on the Web, but these are essentially handcrafted for particular tasks; they have little ability to interact with heterogeneous data and information types. Because we haven&#39;t yet delivered large-scale, agent-based mediation, some commentators argue that the Semantic Web has failed to deliver. We argue that agents can only flourish when standards are well established and that the Web standards for expressing shared meaning have progressed steadily over the past five years. Furthermore, we see the use of ontologies in the e-science community presaging ultimate success for the Semantic Web--just as the use of HTTP within the CERN particle physics community led to the revolutionary success of the original Web. This article is part of a special issue on the Future of AI.</td>
<td class='normal' valign='top'>2006</td>
<td class='normal' valign='top'>NULL</td>
</tr>

<tr>
<td class='normal' valign='top'>8</td>
<td class='normal' valign='top'>D2R Server - Publishing Relational Databases on the Web as SPARQL Endpoints</td>
<td class='normal' valign='top'>The Resource Description Framework and the SPARQL query language provide a standardized way for exposing and linking data sources on the Web. D2R Server is a turn-key solution for making the content of existing, non-RDF databases accessible as SPARQL endpoints. The server takes SPARQL queries from the Web and rewrites them via a mapping into SQL queries against a relational database. This on-the-fly translation allows RDF applications to access the content of large databases without having to replicate them into RDF. D2R Server can be used to integrate existing databases into RDF systems, and to add SPARQL interfaces to database-backed software products. In the talk, we will give an introduction into the D2RQ mapping language, which is used to define mappings between relational and RDF schemata, and demonstrate how D2R Server can be used to extend a WordPress blog with a SPARQL interface.</td>
<td class='normal' valign='top'>2006</td>
<td class='normal' valign='top'>23542</td>
</tr>
</table>
</body></html>
